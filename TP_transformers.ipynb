{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferrorra/m2-mlsd/blob/main/TP_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KeyBERT**\n",
        "\n",
        "KeyBERT est une méthode simple mais efficace pour extraire des mots-clés d'un document en utilisant des embeddings de phrases ou de mots. Voici une explication de son fonctionnement avec un exemple :\n",
        "\n",
        "---\n",
        "\n",
        "## **Document en entrée**\n",
        "Le document d'entrée est :\n",
        "> \"Most microbats use echolocation to navigate and find food.\"\n",
        "\n",
        "Ce texte sera utilisé pour extraire les mots-clés les plus représentatifs.\n",
        "\n",
        "## **Étape 1 : tokenisation**\n",
        "Le document est découpé en différents **tokens** (mots individuels) :\n",
        "- \"most\", \"microbats\", \"use\", \"echolocation\", \"navigate\", \"find\", \"food\", etc.\n",
        "\n",
        "KeyBERT utilise une matrice de comptage pour extraire des n-grams (mono-grammes, bi-grammes, etc.) à partir du texte (voir le premier TP).\n",
        "\n",
        "## **Étape 2 : extraction des embeddings**\n",
        "Chaque **token** ou le document entier est transformé en **embedding**, en utilisant **BERT**.\n",
        "\n",
        "Ces embeddings permettent de représenter les similarités sémantiques entre les mots ou entre le document et les mots.\n",
        "\n",
        "## **Étape 3 : calcul de la similarité cosinus**\n",
        "Pour chaque **token**, on calcule la **similarité cosinus** entre son embedding et celui du document.\n",
        "\n",
        "Exemple :\n",
        "- Pour les mots \"most\" et \"food\", les similarités cosinus par rapport au document peuvent être respectivement de `0.08` et `0.73`. Plus le score est élevé, plus le mot est pertinent par rapport au document.\n",
        "\n",
        "---\n",
        "\n",
        "## **Résultat : extraction des mots-clés**\n",
        "Les mots ayant les similarités cosinus les plus élevées sont considérés comme les mots-clés les plus représentatifs du document.\n",
        "\n",
        "Dans cet exemple :\n",
        "- \"food\" a une similarité élevée (0.73) et est un bon candidat comme mot-clé.\n"
      ],
      "metadata": {
        "id": "zSOwpjLcCja1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "image_url = 'https://cifre.s3.eu-north-1.amazonaws.com/keybert.png'\n",
        "\n",
        "# Afficher l'image.\n",
        "display(Image(url=image_url, width=1000, height=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "_o6MNNUsBxOX",
        "outputId": "37b790d1-f904-4ca2-d4f7-de5fc8e5a7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cifre.s3.eu-north-1.amazonaws.com/keybert.png\" width=\"1000\" height=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keybert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiiymJAz9dd_",
        "outputId": "061b7332-111b-46f9-baf2-b87630b7208e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.8.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.9.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.5.2)\n",
            "Collecting sentence-transformers>=0.3.8 (from keybert)\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (4.12.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Downloading keybert-0.8.5-py3-none-any.whl (37 kB)\n",
            "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers, keybert\n",
            "Successfully installed keybert-0.8.5 sentence-transformers-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qjhGuwQ9Q5R"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "doc = \"\"\"\n",
        "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.\n",
        "It infers a function from labeled training data consisting of a set of training examples.\n",
        "In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).\n",
        "A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples.\n",
        "An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances.\n",
        "This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias).\n",
        "\"\"\"\n",
        "kw_model = KeyBERT()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21LrDY1i9aJc",
        "outputId": "c877d218-f791-41d8-df0f-484091cf917a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('supervised', 0.6676),\n",
              " ('labeled', 0.4896),\n",
              " ('learning', 0.4813),\n",
              " ('training', 0.4134),\n",
              " ('labels', 0.3947),\n",
              " ('supervisory', 0.3297),\n",
              " ('data', 0.3136),\n",
              " ('algorithm', 0.298),\n",
              " ('class', 0.296),\n",
              " ('object', 0.2789)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3IwdXp49y1v",
        "outputId": "f089d977-5a0b-4057-c013-69a4c587b657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('supervised learning', 0.6779),\n",
              " ('supervised', 0.6676),\n",
              " ('signal supervised', 0.6152),\n",
              " ('examples supervised', 0.6112),\n",
              " ('labeled training', 0.6013),\n",
              " ('learning function', 0.5755),\n",
              " ('learning algorithm', 0.5632),\n",
              " ('learning machine', 0.5598),\n",
              " ('machine learning', 0.5555),\n",
              " ('training data', 0.5271)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I32PyXtCy3DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice 1**\n",
        "\n",
        "Utiliser KeyBERT pour extraire les mots-clés de chaque document de BBC News, puis essayer de représenter les topics par les termes les plus fréquents en ne comptant que les mots qui apparaissent souvent dans les documents. Comparez ce résultat avec celui obtenu lors du premier TP."
      ],
      "metadata": {
        "id": "vPoPkBDzEXYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A vous de jouer"
      ],
      "metadata": {
        "id": "WDKmnaQjGajV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice 2**\n",
        "\n",
        "1. Récupérez les vecteurs de ces mots-clés en utilisant le modèle BERT.\n",
        "2. Appliquez ACP et UMAP sur les vecteurs obtenus et colorez les points en fonction de la colonne `topic_id`.\n",
        "3. Interprétez les résultats et comparez-les avec ceux obtenus avec Word2Vec."
      ],
      "metadata": {
        "id": "OM02YpDWFT-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A vous de jouer"
      ],
      "metadata": {
        "id": "BDXRaSzfFS7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice 3**\n",
        "\n",
        "1. Créez les vecteurs des documents en utilisant la somme et la moyenne des tokens.\n",
        "2. Lancez KMeans sur les deux représentations avec 5 clusters. Veillez bien à augmenter le nombre d'itérations et d'initialisations.\n",
        "3. Visualisez les clusters formés avec UMAP et ACP, et interprétez les résultats.\n",
        "4. Visualisez les matrices de confusion et interprétez-les."
      ],
      "metadata": {
        "id": "dM-uL_CZIzLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nIkREkhEOfj",
        "outputId": "78ad9635-6637-4965-b895-2767abcc3402"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Charger le modèle.\n",
        "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
        "\n",
        "def compute_embeddings(text: str,\n",
        "                       model,\n",
        "                       mode: str = 'mean'):\n",
        "    \"\"\"\n",
        "    Calcule les embeddings d'un texte en utilisant Sentence Transformers.\n",
        "\n",
        "    Args:\n",
        "        text: Le texte pour lequel calculer les embeddings.\n",
        "        model: Le modèle Sentence Transformer à utiliser.\n",
        "        mode: 'mean' pour calculer la moyenne des embeddings ou 'sum' pour calculer la somme.\n",
        "\n",
        "    Returns:\n",
        "        numpy.array: Les embeddings calculés.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extraire les embeddings des tokens.\n",
        "    token_embeddings = model.encode(text, output_value='token_embeddings')\n",
        "\n",
        "    # Calculer la somme ou la moyenne des embeddings en fonction du mode.\n",
        "    if mode == 'sum':\n",
        "        result = np.sum(np.array(token_embeddings), axis=0)\n",
        "    elif mode == 'mean':\n",
        "        result = np.mean(np.array(token_embeddings), axis=0)\n",
        "    else:\n",
        "        raise ValueError(\"Le mode doit être 'sum' ou 'mean'.\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "xNZBqnqAJCmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Un exemple d'utilisation.\n",
        "compute_embeddings(\"Most microbats use echolocation to navigate and find food\",\n",
        "                   model,\n",
        "                   mode=\"mean\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGKxN3wmJJaD",
        "outputId": "5ddb34d5-64d1-4b2c-a048-e1ef0f19683b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.98215619e-01, -4.52998668e-01,  4.13115248e-02, -4.18869853e-01,\n",
              "       -2.55439937e-01, -4.89993721e-01,  5.78509748e-01, -5.18633127e-01,\n",
              "        2.68343002e-01,  2.27568552e-01,  3.65751475e-01, -1.01040006e-01,\n",
              "       -3.52763534e-01, -1.39901847e-01,  1.84774578e-01, -5.09136796e-01,\n",
              "        6.94051981e-01, -4.04877871e-01,  1.66499346e-01, -4.66199405e-02,\n",
              "        9.31270793e-02,  3.48095247e-03, -1.57704353e-01, -2.04838663e-01,\n",
              "       -3.76696372e-03, -4.17477489e-02, -6.76662385e-01, -3.61268967e-01,\n",
              "       -2.37830095e-02, -2.68759161e-01,  3.44662279e-01,  9.03420523e-02,\n",
              "        1.42782435e-01, -3.44587266e-01,  3.31288092e-02, -1.24259003e-01,\n",
              "       -9.43850055e-02, -2.72919629e-02, -6.98226616e-02, -1.97882742e-01,\n",
              "       -1.68647870e-01,  1.21065816e-02,  2.84547448e-01, -1.35192379e-01,\n",
              "       -6.25326633e-02, -3.32675129e-01, -5.95693171e-01,  1.40114605e-01,\n",
              "        3.27405483e-01, -4.07840498e-02, -6.19338810e-01, -4.99174930e-02,\n",
              "       -2.99792260e-01,  4.13072221e-02, -1.24174826e-01,  9.47976857e-02,\n",
              "        1.99151531e-01,  2.64564365e-01, -1.00373738e-01,  2.22810909e-01,\n",
              "        6.71671331e-01, -1.31129667e-01,  6.88743740e-02,  6.76763058e-02,\n",
              "       -9.28368941e-02, -3.53831798e-01, -5.77094071e-02,  3.03591806e-02,\n",
              "        4.60315913e-01, -2.30572790e-01, -1.79268613e-01, -2.05049515e-01,\n",
              "       -3.17421019e-01,  3.51769984e-01,  1.64334714e-01,  2.78125107e-01,\n",
              "        1.08947404e-01, -9.44792151e-01, -3.96613590e-02, -5.22222742e-02,\n",
              "       -3.49719912e-01,  3.24760899e-02, -1.52674139e-01,  2.04079092e-01,\n",
              "       -1.55272171e-01,  3.71816486e-01, -1.64173439e-01,  8.64735395e-02,\n",
              "       -1.42418697e-01,  2.47952983e-01, -3.26106369e-01, -2.27121100e-01,\n",
              "       -4.85664532e-02,  4.15312685e-02, -1.76076487e-01, -3.15640658e-01,\n",
              "       -1.83311135e-01, -2.58360684e-01,  6.06810689e-01,  3.12672287e-01,\n",
              "        1.36890132e-02, -7.04032509e-03,  2.22707063e-01, -1.40942171e-01,\n",
              "       -1.19298659e-01, -5.32171488e-01, -8.50381404e-02,  3.06771815e-01,\n",
              "        7.69671619e-01,  9.60085467e-02, -3.33488733e-01,  5.78048348e-01,\n",
              "        3.93130295e-02, -1.94043145e-01, -4.54483539e-01, -2.21569031e-01,\n",
              "       -2.09600627e-01,  5.83254173e-02,  1.89545862e-02,  4.06552479e-02,\n",
              "        4.75466345e-03, -3.51034313e-01,  6.64177015e-02,  3.31148654e-02,\n",
              "        3.98151457e-01, -1.87930867e-01, -1.07724547e-01,  1.51150942e-01,\n",
              "        4.77386522e-04, -5.18542938e-02,  4.35514838e-01,  6.77760392e-02,\n",
              "        4.68434662e-01, -1.32997781e-01,  1.83293864e-01, -1.15918763e-01,\n",
              "        3.02645475e-01, -3.76049161e-01, -1.86884236e-02,  3.11670959e-01,\n",
              "       -3.34126681e-01, -4.44458947e-02,  1.21643201e-01, -4.16733593e-01,\n",
              "       -1.21347152e-01,  6.35145009e-01, -4.04280990e-01, -5.93062639e-01,\n",
              "       -5.52592695e-01, -4.18801278e-01,  4.07508314e-01, -6.37300730e-01,\n",
              "        4.06903803e-01, -2.30314583e-01,  1.11657880e-01,  9.26840678e-02,\n",
              "        4.83494475e-02,  5.27073085e-01,  2.53029287e-01, -9.23332497e-02,\n",
              "       -6.50785208e-01,  2.32694149e-02, -2.40875911e-02, -2.99820676e-02,\n",
              "        6.38873875e-01, -3.80518854e-01, -2.75220364e-01, -1.07005633e-01,\n",
              "       -6.32005453e-01,  1.29333198e-01, -4.13502306e-01, -1.92984212e-02,\n",
              "       -6.38547182e-01, -5.95780015e-01, -4.63866025e-01,  1.52334347e-01,\n",
              "       -3.46014559e-01,  1.13887466e-01, -1.18536815e-01, -3.26493919e-01,\n",
              "        8.53502229e-02,  4.68003392e-01,  7.40443051e-01,  1.16635412e-01,\n",
              "        1.29198685e-01, -4.49728630e-02,  3.41405988e-01,  2.52196401e-01,\n",
              "       -5.48356771e-02,  1.39275238e-01,  3.39494795e-01, -1.12356052e-01,\n",
              "        2.86754698e-01,  4.78737205e-01,  1.37050048e-01, -5.41807618e-03,\n",
              "        3.49990547e-01,  1.09923907e-01,  2.09242925e-01,  1.46685645e-01,\n",
              "        2.17309088e-01, -4.62471962e-01,  9.19301361e-02,  4.65855487e-02,\n",
              "       -3.11705112e-01, -4.62101549e-02, -2.41924629e-01, -6.48051649e-02,\n",
              "        4.89244729e-01,  3.60504836e-01, -4.27272171e-01,  2.60984361e-01,\n",
              "       -8.82481635e-01, -1.13330849e-01,  4.40656692e-02, -8.17143321e-01,\n",
              "       -3.58956456e-01,  1.83667317e-01, -6.41814411e-01,  1.67166889e-01,\n",
              "       -2.50858422e-02, -3.84717792e-01, -5.52727520e-01, -2.82259077e-01,\n",
              "       -7.04906464e-01, -1.88410154e-03, -3.99258673e-01,  2.45937154e-01,\n",
              "       -9.49378833e-02,  7.14186355e-02,  3.31095636e-01, -4.93208505e-02,\n",
              "        1.43399462e-01,  1.76337026e-02, -5.00371635e-01,  3.11945736e-01,\n",
              "        3.34540337e-01,  7.76758566e-02, -3.97252917e-01,  4.45641249e-01,\n",
              "        4.20073241e-01, -2.07087442e-01, -4.08289641e-01, -1.80141814e-02,\n",
              "       -5.86423576e-01, -3.47265542e-01, -3.70898753e-01,  5.48523962e-01,\n",
              "        5.16332611e-02,  2.37215608e-01, -1.61250591e-01,  3.62103432e-02,\n",
              "       -6.18142225e-02, -1.91628769e-01, -5.96560463e-02, -1.62647292e-02,\n",
              "        1.24280229e-01,  7.68566281e-02,  8.31931755e-02,  2.39251956e-01,\n",
              "       -6.96329996e-02,  5.01700461e-01,  1.19276129e-01,  9.88310203e-02,\n",
              "        1.67008609e-01, -1.95939928e-01, -2.59194106e-01,  1.39503628e-01,\n",
              "       -5.16086280e-01,  1.31109748e-02, -1.94429740e-01, -1.17947832e-01,\n",
              "        9.08789262e-02,  4.39623713e-01,  7.36917853e-01, -3.51202667e-01,\n",
              "        2.28767805e-02, -5.61083257e-01, -3.96521658e-01,  6.70039535e-01,\n",
              "        8.10357742e-03,  1.40626639e-01,  4.75711189e-02,  2.12947473e-01,\n",
              "       -9.21890959e-02, -2.73613334e-01,  1.23161888e-02,  2.39484251e-01,\n",
              "        1.96130857e-01, -1.51050776e-01,  4.19302881e-01,  3.73579890e-01,\n",
              "        2.53601298e-02, -1.59711078e-01,  2.10379466e-01, -4.90644462e-02,\n",
              "        8.29918027e-01,  2.01239869e-01,  3.54060978e-01,  3.24749947e-01,\n",
              "       -3.90789926e-01, -1.39883772e-01, -3.57308924e-01,  7.11618125e-01,\n",
              "        6.98326854e-03,  4.52828825e-01,  3.02168936e-01,  8.98357630e-02,\n",
              "        7.26429164e-01, -2.60536075e-01, -3.21891345e-02,  5.27861714e-03,\n",
              "        1.23870373e-01,  4.35176998e-01, -2.15011507e-01,  4.62120265e-01,\n",
              "        2.04519182e-01, -7.41035417e-02,  3.65538865e-01,  2.22465083e-01,\n",
              "        3.54552940e-02, -5.84218204e-01,  4.43962961e-01,  3.64492685e-01,\n",
              "       -1.73384428e-01,  9.78522673e-02,  1.01018831e-01,  1.78645924e-01,\n",
              "       -2.13294034e-03, -1.07654169e-01,  1.00873848e-02, -1.75247058e-01,\n",
              "       -4.98761572e-02,  3.21626544e-01,  6.38695300e-01, -3.89346443e-02,\n",
              "        2.64580637e-01, -1.25645816e-01, -6.10861242e-01, -2.23044947e-01,\n",
              "       -8.78437683e-02,  5.06439269e-01,  8.93633533e-03, -1.19539037e-01,\n",
              "       -1.77455410e-01,  1.65957928e-01, -5.73161125e-01, -2.27359027e-01,\n",
              "       -2.03978926e-01,  6.13239668e-02,  3.10003817e-01,  5.14091849e-01,\n",
              "       -1.44249037e-01, -8.01375285e-02, -2.75013506e-01,  3.50397766e-01,\n",
              "       -4.42959607e-01,  5.44895008e-02, -8.89786556e-02,  5.10804877e-02,\n",
              "        7.36201853e-02, -2.03340143e-01, -3.61476392e-01,  2.26913303e-01,\n",
              "       -1.49200216e-01,  2.70752937e-01, -4.32953425e-02,  1.24411270e-01,\n",
              "        1.53581779e-02,  6.11755610e-01,  3.69929492e-01, -6.62444113e-03,\n",
              "        2.58004516e-01, -2.38063540e-02, -6.57480210e-02,  2.59939134e-01,\n",
              "        4.11659211e-01, -2.44715989e-01,  5.27504802e-01,  6.48079455e-01,\n",
              "        1.04650952e-01,  4.39417839e-01, -1.66957796e-01,  1.11846551e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice 4**\n",
        "1. Utiliser PCA et UMAP pour réduire la dimension des embeddings avant le clustering.\n",
        "2. Reprenez les étapes 2, 3 et 4 de l'exercice 3."
      ],
      "metadata": {
        "id": "q5k1MDb0SXoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A vous de jouer."
      ],
      "metadata": {
        "id": "8VIm3EoESdGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice 5: Clustering avec le modèle InstructorEmbedding**\n",
        "\n",
        "1. Créer des paires **instruction-texte** pour chaque article, puis générer les embeddings (voir code).\n",
        "2. Utiliser PCA et UMAP pour réduire la dimensionnalité des embeddings avant le clustering.\n",
        "3. Appliquer l'algorithme de KMeans pour regrouper les articles en clusters.\n",
        "4. Visualiser les clusters avec ACP et UMAP.\n",
        "5. Affichez la matrice de confusion et interprétez-la.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JKPuIQGBNI7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install InstructorEmbedding"
      ],
      "metadata": {
        "id": "h3JgCfRhNHOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour charger le modèle.\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "model = INSTRUCTOR('hkunlp/instructor-large')"
      ],
      "metadata": {
        "id": "p9C6otbXQsX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation des paires instruction-texte.\n",
        "text_instruction_pairs = [\n",
        "    {\"instruction\": \"Represent the News article:\", \"text\": article} for article in bbc_df['text']\n",
        "]\n",
        "\n",
        "# Génération des embeddings.\n",
        "embeddings = model.encode([[pair['instruction'], pair['text']] for pair in text_instruction_pairs])"
      ],
      "metadata": {
        "id": "hE1WCfQHRI4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice 6**\n",
        "Essayez de trouver une instruction plus pertinente qui permet d'améliorer le clustering.\n"
      ],
      "metadata": {
        "id": "6eGdEVJZ9dCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A vous de jouer."
      ],
      "metadata": {
        "id": "5YCr6jNsR_cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice 7**\n",
        "Reprenez les étapes 1, 2, 3 et 4 de l'exercice 4 sur les deux représentations suivantes : la première obtenue avec GPT et la deuxième avec JoSE.\n"
      ],
      "metadata": {
        "id": "YSoUlRiQCvsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import requests\n",
        "\n",
        "# Download the dataset.\n",
        "url = \"https://cifre.s3.eu-north-1.amazonaws.com/bbc_dataset.pickle\"\n",
        "response = requests.get(url)\n",
        "\n",
        "bbc_gpt = pickle.loads(response.content)\n",
        "\n",
        "bbc_gpt.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F6wd4gnCxlo",
        "outputId": "5c9b0a8f-b049-403c-f783-50769ba7023b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['embeddings', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "url_mat = \"https://cifre.s3.eu-north-1.amazonaws.com/BBC_JOSE.mat\"\n",
        "response_mat = requests.get(url_mat)\n",
        "with open(\"./BBC_JOSE.mat\", 'wb') as f:\n",
        "    f.write(response_mat.content)\n",
        "bbc_jose = loadmat(\"./BBC_JOSE.mat\")\n",
        "\n",
        "bbc_jose.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEtsAu2HDpKk",
        "outputId": "20ba9794-5eb6-4d23-d771-91cafd44fb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['__header__', '__version__', '__globals__', 'embeddings', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A vous de jouer."
      ],
      "metadata": {
        "id": "oXW5nX1EGj1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instructions pour le rendu**\n",
        "\n",
        "Pour le rendu, vous devez le déposer sur le drive suivant : https://drive.google.com/drive/folders/1ao78S52_D5vpbNj9h4QD1jBlY0_jODq8?usp=drive_link(URL_DU_DRIVE).\n",
        "\n",
        "\n",
        "### **Veuillez préciser les noms et prénoms des monomes/binomes dans la première cellule du notebook.**"
      ],
      "metadata": {
        "id": "GAQwgjVzFjz6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkHIBP7IE7oJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}